# PEARL direction target reward configs

# General setup
# -------------
# Number of tasks for meta-train
train_tasks: 2

# Number of tasks for meta-test
test_tasks: 2

# Dimension of the latent context vector
latent_dim: 5

# Dimension of hidden units in neural networks
hidden_dim: 300

# PEARL setup
# -----------
pearl_params:
    # Number of training iterations
    num_iterations: 1000
    # Number of sampled tasks to collect data for each iteration
    num_sample_tasks: 5
    # Number of samples collected per task before training
    num_init_samples: 2000
    # Number of samples to collect per task with z ~ prior
    num_prior_samples: 1000
    # Number of samples to collect per task with z ~ posterior
    # that are only used to train the policy and NOT the encoder
    num_posterior_samples: 1000
    # Number of meta-gradient taken per iteration
    num_meta_grads: 1500
    # Number of tasks to average the gradient across
    meta_batch_size: 4
    # Number of samples in the context batch
    batch_size: 256
    # Maximum step for the environment
    max_step: 200
    # How many samples to store
    max_buffer_size: 1000000
    # Number of early stopping conditions
    num_stop_conditions: 3
    # Goal value used to early stopping condition
    stop_goal: 1900

# SAC setup
# ---------
sac_params:
    # Discount factor
    gamma: 0.99
    # Weight on KL divergence term in encoder loss
    kl_lambda: 0.1
    # Number of samples in the RL batch
    batch_size: 256
    # Q-function network's learning rate
    qf_lr: 0.0003
    # Encoder network's learning rate
    encoder_lr: 0.0003
    # Policy network's learning rate
    policy_lr: 0.0003
