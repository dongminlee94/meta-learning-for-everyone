# RL^2 direction target reward configs

# General setup
# -------------
# Number of tasks for meta-train
train_tasks: 2

# Number of tasks sampled from train-task set for meta-test
num_test_tasks: 15

# Number of hidden units in policy networks
policy_hidden_dim: 64

# Number of hidden units in value networks
value_function_hidden_dim: 32

# MAML setup
# ----------
maml_params:
    # Number of training iterations
    num_iterations: 200
    # Number of task samples for training
    num_sample_tasks: 40
    # Number of samples per task to train
    num_samples: 4000
    # Maximum steps for the environment
    max_steps: 200
    # Number of inner adaptation iterations for the MAML algorithm
    num_adapt_epochs: 1
    # Learning rate of inner adaptation
    inner_learning_rate: 0.1
    # Number of early stopping conditions
    num_stop_conditions: 3
    # Goal value used to early stopping condition
    stop_goal: 200

# TRPO setup
# ---------
    # Maximum iters for backtracking line search
    backtrack_iters: 15
    # Exponentially decreasing ratio for line searching step
    backtrack_coeff: 0.8
    # KL-divergence limit
    max_kl: 0.01
pg_params:
    # Learning rate of value_fuction as a baseline fitting
    vf_learning_rate: 0.1
    # Learning iterations of value_fuction as a baseline fitting
    vf_learning_iters: 2000
    # RL discount factor
    gamma: 0.99
    # gae parameters
    lamda: 1.0


