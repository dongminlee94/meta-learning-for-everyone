# RL^2 direction target reward configs

# General setup
# -------------
# Number of tasks for meta-train
train_tasks: 2

# Number of tasks for meta-test
test_tasks: 2

# Number of hidden units in neural networks
hidden_dim: 256

# RL^2 setup
# ----------
rl2_params:
    # Number of training iterations
    num_iterations: 10000
    # Number of samples to train
    num_samples: 800
    # Maximum step for the environment
    max_step: 200
    # Number of early stopping conditions
    num_stop_conditions: 3
    # Goal value used to early stopping condition
    stop_goal: 130

# PPO setup
# ---------
ppo_params:
    # Discount factor
    gamma: 0.99
    # Number of epochs per iteration
    num_epochs: 5
    # Number of minibatch within each epoch
    mini_batch_size: 128
    # PPO clip parameter
    clip_param: 0.3
    # Learning rate of PPO losses
    learning_rate: 0.0001

# TRPO setup
# ---------
trpo_params:
    # Discount factor
    gamma: 0.99
    # Number of minibatch within each epoch
    mini_batch_size: 128
    # Maximum iters for backtracking line search
    backtrack_iters: 30
    # Exponentially decreasing ratio for line searching step
    backtrack_coeff: 0.2
    # KL-divergence limit
    max_kl: 0.01
    # Learning rate of TRPO losses
    learning_rate: 0.0001
    # Learning rate of value_fuction as a baseline fitting
    vf_learning_rate: 0.01
    # Learning iterations of value_fuction as a baseline fitting
    vf_learning_iters: 1
    # RL discount factor
    gamma: 0.99
    # gae parameters
    lamda: 1.0